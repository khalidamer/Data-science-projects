{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Ground Truth and Object Detection\n",
    "## Workshop Guide\n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction to dataset](#introduction)\n",
    "2. [Labeling with SageMaker Ground Truth](#groundtruth)\n",
    "3. [Reviewing labeling results](#review)\n",
    "4. [Training an Object Detection model](#training)\n",
    "5. [Review of Training Results](#review_training)\n",
    "6. [Model Tuning](#model_tuning)\n",
    "7. [Cleanup](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap embedded images\n",
    "\n",
    "Download images used in the rest of this guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = 'sagemaker-us-east-1-821202580419'\n",
    "PREFIX = 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reload the notebook (close/reopen for JupyterLab) so that the instruction images in the rest of this notebook become visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction to dataset\n",
    "In this workshop we will use a dataset from the [inaturalist.org](inaturalist.org) This dataset contains 500 images of bees that have been uploaded by inaturalist users for the purposes of recording the observation and identification. We only used images that their users have licensed under [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/) license. For your convenience, we have placed the dataset in S3 in a single zip archive here: http://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/DIG-TF-200-MLBEES-10-EN/dataset.zip \n",
    "\n",
    "First, download and unzip the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget http://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/DIG-TF-200-MLBEES-10-EN/dataset.zip \n",
    "!unzip -qo dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive contains the following structure: 500 `.jpg` image files, a manifest file (to be explained later) and 10 test images in the `test` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!unzip -l dataset.zip | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's upload this dataset to your own S3 bucket in preparation for labeling and training using Amazon SageMaker. For this workshop we will be using `us-west-2` region, so your bucket needs to be in this region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync --exclude=\"*\" --include=\"[0-9]*.jpg\" . s3://$BUCKET/$PREFIX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling with SageMaker Ground Truth <a name=\"groundtruth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to run your first labeling with Amazon SageMaker Ground Truth. We have already labeled the larger dataset, and you will be using this labeled dataset to train your model. This exercise is meant to only demonstrate how you would use SageMaker Ground Truth to label your own datasets, so you will only get label  small sample labeled. Below we walk through the four steps for creating and kicking off a labeling job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Navigate to the SageMaker Ground Truth console.\n",
    "\n",
    "In the AWS Management Console, navigate to Amazon SageMaker and then select 'Labeling jobs' option under the 'Ground Truth' menu. Remember to make sure you are in the Oregon region.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step1a.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "To kick off a Labeling, click the 'Create labeling job' button.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step1b.png\" width=\"600\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up input and output dataset locations\n",
    "\n",
    "Let's begin by giving a name to the labeling job. Since this labeling job is only meant to be a sample, let's name it 'Sample-labeling-job'. \n",
    "\n",
    "Then, let's give the path to the input dataset location. A labeling job takes an input manifest as the input dataset. An input manifest effectively points to the dataset objects in the dataset.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step2a.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Realize that we have not yet created the input manifest. Instead above, we only uploaded 500 images. Fortunately, SageMaker Ground Truth provides a cool feature that creates the input manifest for you. \n",
    "\n",
    "To use this feature, first select 'Automated data setup', then click on the 'Complete Data Setup' link and provide the path to the uploaded images. \n",
    "\n",
    "Then, select 'Image' as the 'Data type'. \n",
    "\n",
    "Then, we will provide the IAM Role that allows SageMaker Ground Truth to read and write to your S3 locations. The easiest option is 'Create a new role' and specifically provide access to the your S3 bucket.\n",
    "\n",
    "There are currently 500 images in our input dataset. We do not want to send all 500 images to the labeling job (for time and cost reasons). Instead, we want to randomly sample 2% of the images (or 10 images). SageMaker Ground Truth makes it easy to perform this sampling.\n",
    "\n",
    "Click on 'Additional configuration', and click on the 'Random sample' option. Next, indicate you want to sample 2% of the dataset. Then to complete the process, click on 'Create subset'. This will create a new input manifest, and once it is created, click 'Use this subset'. \n",
    "\n",
    "<img src=\"guide/Tutorial-Step2d.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "Make sure to click on the \"Use this subset\" button once the subset has been created.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select task type and worker type\n",
    "\n",
    "Under 'Task type', select the 'Bounding box' option. To complete this step, click 'Next' at the bottom of the page.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step3a.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "For this labeling job, we want to use the **'Private'** worker option. This option will direct the labeling job to an internally defined workforce. For the purposes of this workshop we will use our own contact details to set ourselves up as the workforce. \n",
    "\n",
    "<img src=\"guide/Tutorial-Private.png\" width=\"600\" border=\"3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the task instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most important step as it will directly impact the accuracy of the labels generated. We have discussed this topic in-depth in this blog post: https://aws.amazon.com/blogs/machine-learning/create-high-quality-instructions-for-amazon-sagemaker-ground-truth-labeling-jobs/\n",
    "\n",
    "Start with your primary call-to-action in the header; here we use: \"Draw a bounding box around the bee in this image.\" Then, let's define the label category on the right-hand side. This is \"bee\" in our case. \n",
    "\n",
    "Next, we want to provide visual examples of both good and bad labels. We have pre-created these examples, and you can directly insert these examples by clicking on the image icon and then pointing to the below public URLs:\n",
    "\n",
    "Good Example - https://s3.us-west-2.amazonaws.com/sagemaker-remars/bee-good-5535715.jpg \n",
    "\n",
    "Bad Example - https://s3.us-west-2.amazonaws.com/sagemaker-remars/bee-bad-5535715.jpg\n",
    "\n",
    "Then, we suggest providing some helper text to guide the workers for both good and bad examples. Once done select the \"Preview\" button if you like to view the page presented to the workers.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step4a.png\" width=\"600\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "\n",
    "By now you should have received an email inviting you to log into the labeling UI. Click on the link provided and login - you will be asked to change your password. Once in, start labeling.\n",
    "\n",
    "<img src=\"guide/Turorial-PrivateEmail.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "If instead of Private workforce you chose Public, the labeling will be performed by the Amazon Mechanical Turk public workforce, which is globally distributed, 24/7 available. With the public workforce it may take some time before all images are labeled even if it's a small subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing labeling results\n",
    "<a name=\"reviewing\"></a>\n",
    "\n",
    "**WARNING: Pick OPTION 1 or OPTION 2 cells below**\n",
    "\n",
    "If your labeling job has not finished (in case you chose the Public workforce), we can review results of the previously completed labeling job called ```bees-500``` using the provided augmented manifest file ```output.manifest```. In that case run OPTION 1 cell below and skip OPTION 2. Otherwise, skip OPTION 1 and run OPTION 2 to use results of your own labeling job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# OPTION 1: provided labeling results\n",
    "###########\n",
    "labeling_job_name = 'bees-500'\n",
    "augmented_manifest_file = 'output.manifest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "###########\n",
    "### OPTION 2: your own labeling job completed\n",
    "###########\n",
    "### Enter the name of your job here\n",
    "\n",
    "```python\n",
    "\n",
    "labeling_job_name = 'Sample-labeling-job'\n",
    "\n",
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "s3_output_path = client.describe_labeling_job(LabelingJobName=labeling_job_name)['OutputConfig']['S3OutputPath'].rstrip('/')\n",
    "augmented_manifest_url = f'{s3_output_path}/{labeling_job_name}/manifests/output/output.manifest'\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    os.makedirs('od_output_data/', exist_ok=False)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree('od_output_data/')\n",
    "\n",
    "# now download the augmented manifest file and display first 3 lines\n",
    "!aws s3 cp $augmented_manifest_url od_output_data/\n",
    "augmented_manifest_file = 'od_output_data/output.manifest'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first few lines of the manifest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing bucket name and prefix to right format\n",
    "outputManifest = open(\"output.manifest\", \"rt\")\n",
    "#read file contents to string\n",
    "manifest = outputManifest.read()\n",
    "#replace all occurrences of the required string\n",
    "replaceStr = BUCKET + '/' + PREFIX\n",
    "manifest = manifest.replace('sagemaker-remars/datasets/na-bees/500', replaceStr)\n",
    "#close the input file\n",
    "outputManifest.close()\n",
    "\n",
    "#open the input file in write mode\n",
    "outputManifest = open(\"output.manifest\", \"wt\")\n",
    "#overrite the input file with the resulting data\n",
    "outputManifest.write(manifest)\n",
    "#close the file\n",
    "outputManifest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -3 $augmented_manifest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all the annotated images. First, let's define a function that displays the local image file and draws over it the bounding boxes obtained via labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "def show_annotated_image(img_path, bboxes):\n",
    "    im = np.array(Image.open(img_path), dtype=np.uint8)\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    colors = cycle(['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w'])\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((bbox['left'],bbox['top'])\n",
    "                                 ,bbox['width']\n",
    "                                 ,bbox['height']\n",
    "                                 ,linewidth=1\n",
    "                                 ,edgecolor=next(colors)\n",
    "                                 ,facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the augmented manifest (JSON lines format) line by line and display the first 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from itertools import islice\n",
    "\n",
    "with jsonlines.open(augmented_manifest_file, 'r') as reader:\n",
    "    for desc in islice(reader, 10):\n",
    "        img_url = desc['source-ref']\n",
    "        img_file = os.path.basename(img_url)\n",
    "        file_exists = os.path.isfile(img_file)\n",
    "\n",
    "        bboxes = desc[labeling_job_name]['annotations']\n",
    "        show_annotated_image(img_file, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='training'></a>\n",
    "## Training an Object Detection Model\n",
    "We are now ready to use the labeled dataset in order to train a Machine Learning model using the SageMaker [built-in Object Detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html).\n",
    "\n",
    "For this, we would need to split the full labeled dataset into a training and a validation datasets. Out of the total of 500 images we are going to use 400 for training and 100 for validation. The algorithm will use the first one to train the model and the latter to estimate the accuracy of the model, trained so far. The augmented manifest file from the previously run full labeling job was included in the original zip archive as `output.manifest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with jsonlines.open('output.manifest', 'r') as reader:\n",
    "    lines = list(reader)\n",
    "    # Shuffle data in place.\n",
    "    np.random.shuffle(lines)\n",
    "    \n",
    "dataset_size = len(lines)\n",
    "num_training_samples = round(dataset_size*0.8)\n",
    "\n",
    "train_data = lines[:num_training_samples]\n",
    "validation_data = lines[num_training_samples:]\n",
    "\n",
    "augmented_manifest_filename_train = 'train.manifest'\n",
    "\n",
    "with open(augmented_manifest_filename_train, 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "\n",
    "augmented_manifest_filename_validation = 'validation.manifest'\n",
    "\n",
    "with open(augmented_manifest_filename_validation, 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        \n",
    "print(f'training samples: {num_training_samples}, validation samples: {len(lines)-num_training_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's upload the two manifest files to S3 in preparation for training. We will use the same bucket you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defines paths for use in the training job request.\n",
    "pfx_training = PREFIX + '/training' if PREFIX else 'training'\n",
    "# Defines paths for use in the training job request.\n",
    "s3_train_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_train)\n",
    "s3_validation_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_validation)\n",
    "\n",
    "!aws s3 cp train.manifest s3://$BUCKET/$pfx_training/\n",
    "!aws s3 cp validation.manifest s3://$BUCKET/$pfx_training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "# Create a train data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.\n",
    "train_data = TrainingInput(s3_train_data_path,\n",
    "                                        distribution='FullyReplicated',\n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'bees-500'],\n",
    "                                        input_mode='Pipe',\n",
    "                                        record_wrapping='RecordIO') \n",
    "\n",
    "# Create a validation data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.\n",
    "validation_data = TrainingInput(s3_validation_data_path,\n",
    "                                        distribution='FullyReplicated',\n",
    "                                        content_type='application/x-recordio',\n",
    "                                        s3_data_type='AugmentedManifestFile',\n",
    "                                        attribute_names=['source-ref', 'bees-500'],\n",
    "                                        input_mode='Pipe',\n",
    "                                        record_wrapping='RecordIO') \n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='console'></a>\n",
    "\n",
    "### Console option\n",
    "\n",
    "#### Step 1: SageMaker Training Job wizard\n",
    "\n",
    "First, click on the \"Training Jobs\" link in the SageMaker Nav bar.\n",
    "\n",
    "<img src=\"guide/nav_training_jobs.png\" width=\"300\" border=\"3\">\n",
    "\n",
    "Then, click on \"Create training job\" button.\n",
    "\n",
    "<img src=\"guide/create_training_job.png\" width=\"800\" border=\"3\">\n",
    "\n",
    "#### Step 2: Enter basic details\n",
    "\n",
    "Next, enter training job name, e.g. `bees-detection-resnet`, create or select an existing role, leave the default `Amazon SageMaker built-in algorithm` and choose `Object Detection` from the drop-down.\n",
    "\n",
    "<img src=\"guide/job_details_1.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Scroll down to select Pipe input mode (data is streamed from S3) and pick the `ml.g4dn.xlarge` instance for training.\n",
    "\n",
    "<img src=\"guide/job_details_2.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "#### Step 3: Hyperparameters\n",
    "\n",
    "Scroll down even further to specify algorithm hyperparameters. The meaning of hyperparameters is available in [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-api-config.html). First, we are going to select `resnet-50` instead of the default `vgg-16` for the `base_network`, then select `1` (means true) for `use_pretrained_model` (we will start with a pretrained model), enter 1 for `num_classes` (we only have one class - bees), choose `100` for the number of training `epochs` and change `mini_batch_size` to `1` (one image per batch).\n",
    "\n",
    "<img src=\"guide/job_details_3.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Keep scrolling to specify more hyperparameters. Enter the `num_training_samples` - it was 400 for our dataset. Then fill the remaining hyperparameters related to early stopping condition as shown below. This means that we might stop training even before the 100 epochs are completed (but not earlier than 50 epochs) when the validation accuracy metric mAP fails to improve by 0.01 in 5 epochs.\n",
    "\n",
    "<img src=\"guide/job_details_4.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "#### Step 4: Specify training/validation datasets\n",
    "\n",
    "Next, let's specify where the training algorithm will read the training and validation datasets (aka channels). Fill in the information as shown in the screenshots below. The S3 location of manifests files should be the same that you used above when uploading them to S3 - replace `s3://sagemaker-remars/training/` with your own prefix.\n",
    "\n",
    "<img src=\"guide/job_details_5.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "Click `Add Channel` in order to add a validation channel. Make sure to type the name `validation` in the `Channel name` box.\n",
    "\n",
    "<img src=\"guide/job_details_6.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "#### Step 5: Kick off the training job\n",
    "\n",
    "Now, let's specify the S3 output location where the training algorithm will save the trained model artifacts. Once again your bucket and prefix will be different. Finally, click the `Create training job` button to start the training!\n",
    "\n",
    "<img src=\"guide/job_details_7.png\" width=\"800\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progess of the training job, you can refresh the console or repeatedly evaluate the following cell. When the training job status reads `'Completed'`, move on to the next part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(region=boto3.Session().region_name\n",
    "                                               ,framework= 'object-detection'\n",
    "                                               , version ='1'\n",
    "                                               , instance_type='ml.p3.2xlarge')\n",
    "s3_output_path = 's3://{}/{}/output'.format(BUCKET, pfx_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {'num_classes':1\n",
    "         ,'base_network':'resnet-50'\n",
    "         ,'use_pretrained_model':1\n",
    "         ,'epochs':100\n",
    "         ,'mini_batch_size':1\n",
    "         ,'num_training_samples':400\n",
    "         ,'early_stopping':True\n",
    "         ,'early_stopping_min_epochs':50\n",
    "         ,'early_stopping_patience':5\n",
    "         ,'early_stopping_tolerance':0.01\n",
    "         ,\"learning_rate\": 0.001\n",
    "         ,\"lr_scheduler_factor\": 0.1\n",
    "         ,\"optimizer\": \"sgd\"\n",
    "         ,\"momentum\": 0.9\n",
    "         ,\"weight_decay\": 0.0005\n",
    "         ,\"overlap_threshold\": 0.5\n",
    "         ,\"nms_threshold\": 0.45\n",
    "         ,\"image_shape\": 300\n",
    "         ,\"label_width\": 350\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object set to using \"Pipe\" mode.\n",
    "model = sagemaker.estimator.Estimator(training_image\n",
    "                                      ,role\n",
    "                                      ,instance_count=1\n",
    "                                      ,instance_type='ml.p3.2xlarge'\n",
    "                                      ,max_run = 360000\n",
    "                                      ,input_mode = 'Pipe'\n",
    "                                      ,output_path=s3_output_path\n",
    "                                      ,sagemaker_session=sess\n",
    "                                      ,hyperparameters=hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model.\n",
    "model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
    "# In the above console screenshots the job name was 'bees-detection-resnet'.\n",
    "# But if you used Python to kick off the training job,\n",
    "# then 'training_job_name' is already set, so you can comment out the line below.\n",
    "#training_job_name = 'bees-detection-resnet'\n",
    "training_job_name = 'object-detection-2021-04-20-15-36-08-976'\n",
    "\n",
    "training_info = client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "print(\"Training job status: \", training_info['TrainingJobStatus'])\n",
    "print(\"Secondary status: \", training_info['SecondaryStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='review_training'></a>\n",
    "\n",
    "## Review of Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the SageMaker model out of model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.strftime('-%Y-%m-%d-', time.gmtime())\n",
    "model_name = training_job_name + '-model' #+ timestamp\n",
    "\n",
    "training_image = training_info['AlgorithmSpecification']['TrainingImage']\n",
    "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "The next cell creates an endpoint that can be validated and incorporated into production applications. This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def end_conf(model_name,training_job_name):\n",
    "    timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "    endpoint_config_name = training_job_name + '-epc' #+ timestamp\n",
    "    endpoint_config_response = client.create_endpoint_config(\n",
    "        EndpointConfigName = endpoint_config_name,\n",
    "        ProductionVariants=[{\n",
    "            'InstanceType':'ml.m4.xlarge',\n",
    "            'InitialInstanceCount':1,\n",
    "            'ModelName':model_name,\n",
    "            'VariantName':'AllTraffic'}])\n",
    "\n",
    "    print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "    print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))\n",
    "    return endpoint_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def endpoint(training_job_name,endpoint_config_name):\n",
    "    timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "    endpoint_name = training_job_name + '-ep' #+ timestamp\n",
    "    print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "    endpoint_params = {\n",
    "        'EndpointName': endpoint_name,\n",
    "        'EndpointConfigName': endpoint_config_name,\n",
    "    }\n",
    "    endpoint_response = client.create_endpoint(**endpoint_params)\n",
    "    print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "    # get the status of the endpoint\n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "    print('EndpointStatus = {}'.format(status))\n",
    "    return endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = end_conf(model_name,training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = endpoint(training_job_name,endpoint_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invoke the deployed endpoint to detect bees in the 10 test images that were inside the `test` folder in `dataset.zip` which you downloaded at the beginning of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "test_images = glob.glob('test/*')\n",
    "print(*test_images, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that converts the prediction array returned by our endpoint to the bounding box structure expected by our image display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prediction_to_bbox_data(image_path, prediction):\n",
    "    class_id, confidence, xmin, ymin, xmax, ymax = prediction\n",
    "    width, height = Image.open(image_path).size\n",
    "    bbox_data = {'class_id': class_id,\n",
    "               'height': (ymax-ymin)*height,\n",
    "               'width': (xmax-xmin)*width,\n",
    "               'left': xmin*width,\n",
    "               'top': ymin*height}\n",
    "    return bbox_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each of the test images, the following cell transforms the image into the appropriate format for realtime prediction, repeatedly calls the endpoint, receives back the prediction, and displays the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Call SageMaker endpoint to obtain predictions\n",
    "def get_predictions_for_img(runtime_client, endpoint_name, img_path):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                       ContentType='application/x-image', \n",
    "                                       Body=payload)\n",
    "\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until the status has changed\n",
    "client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for test_image in test_images:\n",
    "    results.append(get_predictions_for_img(runtime_client, endpoint_name, test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c in [0.1,0.2,0.3,0.4,0.5]:\n",
    "    confidence_threshold = c\n",
    "    best_n = 1\n",
    "    print('#'*50)\n",
    "    print('confidence_threshold equal:{}'.format(confidence_threshold))\n",
    "    print('#'*50)\n",
    "    for i, result in enumerate(results):\n",
    "        # display the best n predictions with confidence > confidence_threshold\n",
    "        predictions = [prediction for prediction in result['prediction'] if prediction[1] > confidence_threshold]\n",
    "        predictions.sort(reverse=True, key = lambda x: x[1])\n",
    "        bboxes = [prediction_to_bbox_data(test_images[i], prediction) for prediction in predictions[:best_n]]\n",
    "        show_annotated_image(test_images[i], bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning <a name=\"model_tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'weight_decay': ContinuousParameter(0.0005,0.00051)\n",
    "                        ,'learning_rate': ContinuousParameter(1e-4, 0.1)\n",
    "                         ,'optimizer':CategoricalParameter(['sgd', 'adam'])\n",
    "                         ,'momentum': ContinuousParameter(0.9,0.91)\n",
    "                         ,'mini_batch_size':IntegerParameter(8,64)\n",
    "                        }\n",
    "\n",
    "objective_metric_name = 'validation:mAP'\n",
    "\n",
    "tuner = HyperparameterTuner(model,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=1)\n",
    "\n",
    "tuner.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.Session().client('sagemaker')\n",
    "tuning_job_result = client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)\n",
    "is_maximize = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type']\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']\n",
    "print('objective metric is:{} and it is {}'.format(objective_name,is_maximize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_training_job_name = tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model object set to using \"Pipe\" mode.\n",
    "if best_training_job_name:\n",
    "    best_model = sagemaker.estimator.Estimator.attach(best_training_job_name\n",
    "                                          ,sagemaker_session=sess)\n",
    "else:\n",
    "    best_model = model\n",
    "    best_training_job_name = training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = end_conf(best_model,best_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = endpoint(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_deploy = tuner.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait until the status has changed\n",
    "client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results=[]\n",
    "for test_image in test_images:\n",
    "    best_results.append(get_predictions_for_img(runtime_client, endpoint_name, test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in [0.1,0.2,0.3,0.4,0.5]:\n",
    "    confidence_threshold = c\n",
    "    best_n = 1\n",
    "    print('#'*50)\n",
    "    print('confidence_threshold equal:{}'.format(confidence_threshold))\n",
    "    print('#'*50)\n",
    "    for i, result in enumerate(best_results):\n",
    "        # display the best n predictions with confidence > confidence_threshold\n",
    "        predictions = [prediction for prediction in result['prediction'] if prediction[1] > confidence_threshold]\n",
    "        predictions.sort(reverse=True, key = lambda x: x[1])\n",
    "        bboxes = [prediction_to_bbox_data(test_images[i], prediction) for prediction in predictions[:best_n]]\n",
    "        show_annotated_image(test_images[i], bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='cleanup'></a>\n",
    "## Cleanup\n",
    "\n",
    "At the end of the lab we would like to delete the real-time endpoint, as keeping a real-time endpoint around while being idle is costly and wasteful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
